# 04. PyTorch Tensor 슬라이싱 \& 메모리 구조 요약

## 1. 음수 인덱스 슬라이싱

- **음수 인덱스**는 파이썬과 동일하게 뒤에서부터 위치를 셉니다.
- 예시: `a[1:-2]`는 1번 인덱스부터 뒤에서 두 번째 전까지 슬라이싱.

```python
import torch
a = torch.tensor([0, 1, 2, 3, 4, 5])
print(a[1:-2])  # tensor([1, 2, 3])
```


## 2. 역순 슬라이싱(step 음수) 지원 여부

- PyTorch는 `a[::-1]`처럼 **step에 음수(-1)를 넣는 슬라이싱을 지원하지 않습니다.**
- 역순이 필요하면 `torch.flip(a, dims=)`을 사용해야 합니다.

```python
print(torch.flip(a, dims=[^0]))  # tensor([5, 4, 3, 2, 1, 0])
```


## 3. PyTorch에서 step 음수 미지원 이유

- 메모리 연속성(stride) 문제
- autograd(자동 미분) 시스템과의 호환성
- 하드웨어(GPU 등) 효율성
- 명확한 함수 분리로 코드 가독성 향상


## 4. autograd(오토그리드)란?

- **PyTorch의 자동 미분 엔진**
- 연산 그래프를 기록해 `.backward()` 호출 시 미분값(gradient)을 자동 계산
- 사용자는 복잡한 미분 수식 없이 간단하게 학습 가능

```python
x = torch.tensor([1.0, 2.0], requires_grad=True)
y = x ** 2 + 3 * x
z = y.sum()
z.backward()
print(x.grad)  # tensor([5., 7.])
```


## 5. 파이썬 vs PyTorch 슬라이싱 차이

| 구분 | [::-1] 지원 | 설명 |
| :-- | :--: | :-- |
| 파이썬/넘파이 | O | 역순 슬라이싱 지원 |
| PyTorch | X | 에러, torch.flip으로 대체 |

## 6. `[:-1]` vs `[::-1]` 차이

| 슬라이싱 | 의미 | 예시 | 결과 |
| :-- | :-- | :-- | :-- |
| `[:-1]` | 마지막 원소 제외 | a[:-1] | [^1][^2][^3][^4] |
| `[::-1]` | 전체 역순(뒤집기) | a[::-1] | [^5][^4][^3][^2][^1] |

## 7. 2차원/3차원 슬라이싱 예시

```python
import numpy as np

arr = np.array([
    [1, 2, 3, 4],
    [5, 6, 7, 8],
    [9, 10, 11, 12]
])
print(arr[1:3, 1:3])
# [[ 6  7]
#  [10 11]]

tensor_3d = np.array([
    [[1, 2, 3], [4, 5, 6]],
    [[7, 8, 9], [10, 11, 12]]
])
print(tensor_3d[0, 1, 2])    # 6
print(tensor_3d[1, 0, 1])    # 8
print(tensor_3d[-1, -1, -1]) # 12
print(tensor_3d[:, 0, :])    # [[1 2 3], [7 8 9]]
print(tensor_3d[0, :, 0:2])  # [[1 2], [4 5]]
```


## 8. 핵심 정리 표

| 차원 | 접근 방식 | 예시 | 결과 |
| :-- | :-- | :-- | :-- |
| 2차원 | `[행범위, 열범위]` | arr[1:3, 1:3] | 2x2 부분 행렬 |
| 3차원 | `[층, 행, 열]` | tensor_3d[^1][^2] | 스칼라 값(6) |
| 3차원 | `[층범위, 행범위, 열범위]` | tensor_3d[:, 0, :] | 모든 층의 첫째 행 |

> **참고:**
> - 음수 인덱스(-1)는 끝에서부터 셈
> - 슬라이싱 `start:end`는 start 포함, end 제외
> - `:`는 전체 축 선택

# PyTorch 텐서 모양 변경 \& 메모리 구조

## 1. 연속성(Contiguity)이란?

- **연속적 텐서**: 데이터가 메모리 상에 순서대로 저장
- **비연속적 텐서**: 슬라이싱, transpose 등으로 stride가 달라져 인접하지 않게 저장
- `.is_contiguous()`로 확인 가능

```python
a = torch.randn(2, 3)
print(a.is_contiguous())  # True
b = a.transpose(0, 1)
print(b.is_contiguous())  # False
```


## 2. 주요 함수별 메모리 동작

| 함수명 | 주요 동작/목적 | 메모리 복사 발생 가능성 | 연속성 필요 | 특징 및 주의점 |
| :-- | :-- | :-- | :-- | :-- |
| view() | 임의 shape로 변환 | 없음 | 필요 | 연속적일 때만, 복사 없음 |
| reshape() | 임의 shape로 변환 | 있음 | 불필요 | 연속적이면 뷰, 아니면 복사 |
| flatten() | 1차원 평탄화 | 있음 | 불필요 | reshape(-1)과 유사, 항상 1차원 |
| transpose() | 두 차원 위치 맞바꿈 | 없음 | 불필요 | stride만 변경, 뷰 반환, 연속성 깨질 수 있음 |
| squeeze() | 크기 1인 차원 제거 | 없음 | 불필요 | 뷰 반환, 차원 1개 낮아짐 |
| unsqueeze() | 크기 1인 차원 추가 | 없음 | 불필요 | 뷰 반환, 차원 1개 높아짐 |
| stack() | 여러 텐서 새 축으로 쌓기 | 있음 | 불필요 | 입력 텐서 shape 동일 필요, 새로운 텐서 생성 |

메모리 복사 발생 가능성

없음:
함수가 새로운 메모리 공간을 만들지 않고, 기존 텐서의 메모리(주소)를 그대로 공유(뷰)합니다.
즉, 값만 해석 방식(shape, stride 등)이 달라질 뿐, 실제 데이터는 복사되지 않습니다.

있음:
함수가 필요하다면(예: 비연속적이거나 shape이 맞지 않을 때)
기존 텐서의 값을 새로운 메모리 주소로 복사해서 새로운 텐서를 만듭니다.
즉, 원본과 결과 텐서가 서로 다른 메모리 공간(주소)을 가집니다.

연속성 필요

필요:
함수가 동작하려면 텐서의 데이터가 메모리 상에 순서대로(인접하게, contiguous) 저장되어 있어야 합니다.
그렇지 않으면 에러가 발생하며, 이 경우 .contiguous()로 먼저 연속화해야 합니다.

불필요:
함수가 연속적이든 아니든 동작합니다.
연속적이면 뷰(view)로 반환하고, 아니면 내부적으로 복사해서 새로운 연속적 텐서를 만들어 반환합니다.



## 3. 함수별 메모리 동작 상세

- **view()**: 연속적 메모리만 지원, 복사 없음. 비연속적이면 `.contiguous()` 필요
- **reshape()**: 연속적이면 뷰, 아니면 복사 발생
- **flatten()**: reshape(-1)과 유사, 연속적이면 뷰, 아니면 복사
- **transpose()**: stride만 변경, 뷰 반환. 연속성 깨질 수 있음
- **squeeze()/unsqueeze()**: 크기 1인 차원 제거/추가, 뷰 반환
- **stack()**: 여러 텐서를 새로운 차원으로 쌓아 새로운 텐서 생성(복사 발생)


## 4. 팁..

- 성능이 중요하고 텐서가 연속적이면 **view()** 사용
- 연속성 여부가 불확실하거나 에러 없이 바꾸려면 **reshape()** 사용
- 비연속 텐서에서 **view()**를 꼭 써야 한다면 **.contiguous()**로 먼저 연속화
- **transpose, squeeze, unsqueeze**는 stride/shape만 바꿔서 복사 없이 뷰 반환

