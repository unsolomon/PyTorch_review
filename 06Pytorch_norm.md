
---

### L1, L2, L∞(무한대) 노름의 계산 방법과 활용

|        **노름 종류**       |               **계산 공식**               |    **계산 방법**   |                              **주요 활용**                              |                **특징/차이점**                |                                                                       |                             |
| :--------------------: | :-----------------------------------: | :------------: | :-----------------------------------------------------------------: | :--------------------------------------: | --------------------------------------------------------------------- | --------------------------- |
|  **L1 노름**<br>(맨해튼 노름) |        (\|x\|*1 = \sum*{i=1}^n        |      x\_i      |                                  )                                  |             각 원소의 절댓값을 모두 더함             | - L1 정규화, Lasso 회귀<br>- 희소 모델링(Feature selection)<br>- 이상치에 강건한 손실 함수 | - 희소성을 유도<br>- 이상치에 강함      |
| **L2 노름**<br>(유클리드 노름) | $\|x\|_2 = \sqrt{\sum_{i=1}^n x_i^2}$ | 각 원소 제곱의 합에 루트 | - L2 정규화, Ridge 회귀<br>- 거리 기반 알고리즘(KNN, KMeans)<br>- 벡터의 일반적인 크기 측정 | - 직선 거리<br>- 모든 차원에 균등한 패널티<br>- 이상치에 민감 |                                                                       |                             |
|  **L∞ 노름**<br>(최대 노름)  |        (\|x\|\_\infty = \max\_i       |      x\_i      |                                  )                                  |           원소 중 절댓값이 가장 큰 값을 취함           | - 로버스트 최적화<br>- 신경망에서 gradient clipping<br>- 최대 오류 제어                 | - 최대값에 민감<br>- 특정 차원 제약에 유용 |

#### 계산 예시 (벡터 $a = [4, 3]$)

* **L1**: $|4| + |3| = 7$
* **L2**: $\sqrt{4^2 + 3^2} = \sqrt{16 + 9} = 5$
* **L∞**: $\max(|4|, |3|) = 4$

---

### 맨해튼, 유클리드, 코사인 유사도의 차이점 및 활용

|  **유사도 종류**  |                     **계산 공식**                     |                     **해석/특징**                     |                **주요 활용**               |           **언제 사용?**           |                        |                     |
| :----------: | :-----------------------------------------------: | :-----------------------------------------------: | :------------------------------------: | :----------------------------: | ---------------------- | ------------------- |
|  **맨해튼 유사도** |            (\frac{1}{1 + \sum\_{i=1}^n            |                    x\_i - y\_i                    |                   })                   | - 거리의 총합(계단식)<br>- 1에 가까울수록 유사 | - 희소 데이터<br>- 빠른 근사 비교 | - 이상치에 강한 비교가 필요할 때 |
| **유클리드 유사도** | $\frac{1}{1 + \sqrt{\sum_{i=1}^n (x_i - y_i)^2}}$ |          - 직선 거리(최단 경로)<br>- 1에 가까울수록 유사          |  - KNN, KMeans 등<br>- 이미지, 임베딩 간 거리 비교 |      - 실제 거리 기반 판단이 필요할 때      |                        |                     |
|  **코사인 유사도** |        $\frac{x \cdot y}{\|x\|_2 \|y\|_2}$        | - 벡터의 방향(각도) 비교<br>- \[-1, 1] 범위<br>- 1일수록 방향이 같음 | - 텍스트 유사도<br>- 검색/추천 시스템<br>- 희소 벡터 비교 |   - 크기를 무시하고 방향(패턴) 비교가 중요할 때  |                        |                     |

---

### 요약: 언제 어떤 노름/유사도를 써야 하나?

* **L1 노름**: 희소성 유도, 이상치에 강건한 모델이 필요한 경우
  → Lasso 회귀, 희소 feature 선택, 정규화, 로버스트 학습
* **L2 노름**: 일반적인 거리 계산, 모든 특성에 균형 있게 패널티 부여
  → Ridge 회귀, 거리 기반 알고리즘(KNN, SVM), 벡터 정규화
* **L∞ 노름**: 최대 요소 제어, 특정 값이 지나치게 크지 않도록 제한
  → Gradient clipping, 제한 최적화 문제, 안전 제약 조건
* **맨해튼/유클리드 유사도**: 거리 기반 비교. 값이 작을수록 유사
  → 데이터의 분포와 이상치 민감도를 고려해 선택
* **코사인 유사도**: 방향성 기반. 값이 1에 가까울수록 유사
  → 문서/텍스트 유사도, 임베딩 기반 검색, 크기 무시하고 방향 비교할 때



# PyTorch 행렬 곱셈 코드와 차이점 정리

### 1. 행렬 곱셈 코드 예시

```python
import torch

D = torch.tensor([[1, 1, 3],
                  [4, 5, 6],
                  [7, 8, 9]])

E = torch.tensor([[1, 0],
                  [1, -1],
                  [2, 1]])

# 방법 1: matmul
F1 = D.matmul(E)
print('D.matmul(E) =', F1)

# 방법 2: mm
F2 = D.mm(E)
print('D.mm(E) =', F2)

# 방법 3: @ 연산자
F3 = D @ E
print('D @ E =', F3)
```


### 2. 각 행렬 곱셈 함수/연산자의 차이점

| 구분 | 함수/연산자 | 지원 차원 | 특징 및 사용 시기 | 메모리/성능 |
| :-- | :-- | :-- | :-- | :-- |
| 함수 | `matmul()` | 1D, 2D, ND | **가장 범용적**. 벡터, 행렬, 배치 행렬 곱 모두 지원. | 새 메모리 생성 (out 지정 가능) |
| 함수 | `mm()` | 2D × 2D (행렬만) | **2차원 행렬 곱 전용**. 빠르고 단순. | 새 메모리 생성 (out 지정 가능) |
| 연산자 | `@` | 1D, 2D, ND | 파이썬 3.5+의 행렬 곱 연산자. 내부적으로 matmul 호출. | 새 메모리 생성 |

#### 상세 설명

- **matmul()**
    - 1D(벡터), 2D(행렬), N차원(배치 행렬) 곱까지 지원하는 가장 범용적인 함수입니다.
    - 예: (B, N, M) @ (B, M, K) → (B, N, K)
    - 내부적으로 입력 차원에 따라 dot, mm, bmm 등 적절한 연산이 자동 선택됨.
    - 실무에서 다양한 차원의 곱셈을 다룰 때 가장 많이 사용.
- **mm()**
    - 오직 2차원 행렬(2D) 곱만 지원합니다.
    - 벡터(1D)나 배치(3D 이상) 행렬에는 사용할 수 없습니다.
    - 2D 행렬 곱이 명확할 때 가장 빠르고 직관적.
- **@ 연산자**
    - 파이썬 표준 행렬 곱 연산자.
    - 내부적으로 torch.matmul을 호출.
    - 코드 가독성이 좋고, 수식처럼 표현할 수 있어 실무/교육 모두에서 널리 사용.


### 3. 언제 어떤 방식을 쓰는가?

| 상황/목적 | 추천 방식 | 이유/설명 |
| :-- | :-- | :-- |
| 2D 행렬 곱만 할 때 | `mm()` | 가장 빠르고 명확 |
| 다양한 차원 곱 필요할 때 | `matmul()` | 벡터, 행렬, 배치 모두 지원 |
| 가독성/수식형 코드 원할 때 | `@` | 파이썬 표준, 수식처럼 표현 가능, 내부적으로 matmul 사용 |
| 기존 메모리에 결과 저장 | `out=` 옵션 | 메모리 절약, inplace 연산은 지원하지 않음 |

### 4. 실무에서의 활용 ?

- **딥러닝 레이어 연산**: 대부분 `matmul` 또는 `@` 사용 (배치 곱이 많음)
- **행렬 곱만 반복**: `mm()`로 빠르고 단순하게 처리
- **코드 가독성/교육**: `@` 연산자 선호 (수식과 동일하게 보여서 직관적)
- **메모리 관리**: 대용량 연산에서 `out=` 옵션으로 불필요한 메모리 할당 방지


**정리:**

- **matmul**: 범용, 실무에서 가장 많이 사용
- **mm**: 2D 전용, 빠르고 단순
- **@**: 파이썬 표준, 가독성/수식형 코드에 최적
- **모두 새 메모리 생성**, 결과를 기존 텐서에 저장하려면 `out=` 사용