## 경사하강법이란?

- 머신러닝에서 모델의 예측값과 실제값의 차이(손실)를 최소화하기 위해 사용되는 최적화 알고리즘입니다.
- 손실 함수의 값을 가장 작게 만드는 가중치와 바이어스를 찾는 것이 목적입니다.
$$
## 경사하강법의 수학적 공식
$$

### 1. 손실 함수 예시 (평균제곱오차, MSE)

$$
L(w, b) = \frac{1}{n} \sum_{i=1}^{n} [ t_i - (w x_i + b) ]^2
$$

- tᵢ : 실제값  
- xᵢ : 입력값  
- w : 가중치  
- b : 바이어스  
- n : 데이터 개수  

---

### 2. 손실 함수의 기울기(미분값, Gradient)

**가중치 w에 대한 편미분:**

$$
\frac{\partial L(w, b)}{\partial w} = \frac{1}{n} \sum_{i=1}^{n} [ -2 (t_i - y_i) x_i ]
$$

**바이어스 b에 대한 편미분:**

$$
\frac{\partial L(w, b)}{\partial b} = \frac{1}{n} \sum_{i=1}^{n} [ -2 (t_i - y_i) ]
$$

(여기서 yᵢ = w xᵢ + b)

---

### 3. 파라미터(가중치, 바이어스) 업데이트 공식

**가중치 w 업데이트:**

$$
w_{\text{new}} = w - \alpha \frac{\partial L(w, b)}{\partial w}
$$

**바이어스 b 업데이트:**

$$
b_{\text{new}} = b - \alpha \frac{\partial L(w, b)}{\partial b}
$$

- α: 학습률 (learning rate)



## 작동 원리

1. **가중치 초기화**
    - 가중치와 바이어스를 임의의 값으로 설정합니다.
2. **손실 함수 정의**
    - 모델의 예측값과 실제값의 차이를 계산하는 함수를 사용합니다.
    - 예: 평균제곱오차(MSE), 평균절대오차(MAE) 등
3. **기울기(경사) 계산**
    - 손실 함수가 가중치와 바이어스에 대해 얼마나 변하는지, 즉 변화율(기울기)을 계산합니다.
    - 파이토치 등에서는 자동 미분 기능을 통해 쉽게 계산할 수 있습니다.
    - 코드 예시:

```python
loss.backward()
```

4. **가중치 업데이트**
    - 계산된 기울기를 사용해 가중치와 바이어스를 업데이트합니다.
    - 코드 예시:

```python
optimizer.step()
```

5. **경사 초기화**
    - 이전 단계에서 계산된 기울기를 초기화해 다음 반복에서 누적되지 않도록 합니다.
    - 코드 예시:

```python
optimizer.zero_grad()
```


## 학습률(Learning Rate)

- 가중치가 한 번에 얼마나 크게 업데이트되는지를 결정하는 값입니다.
- 학습률이 너무 크면 최적값을 지나칠 수 있고, 너무 작으면 학습이 느려질 수 있습니다.
- 모델과 데이터에 따라 적절한 값을 찾는 것이 중요합니다.


## 경사하강법의 반복적 작동 구조

- 임의의 가중치 값을 선택합니다.
- 현재 가중치에서 손실 함수의 기울기(미분값)를 계산합니다.
- 기울기의 반대 방향으로 가중치를 조금씩 줄여나갑니다.
- 반복적으로 이 과정을 수행하면 손실 함수가 최소가 되는 지점에 도달합니다.
- 바이어스 또한 동일한 방식으로 업데이트합니다.


## 경사하강법 요약

- **반복**: 기울기 계산과 가중치 업데이트를 여러 번 반복하여 손실 함수를 최소화합니다.
- **종료 조건**: 손실 함수의 변화가 거의 없거나, 사전에 정한 반복 횟수(에폭)에 도달하면 종료합니다.


## 경사하강법의 단점

- 데이터셋이 매우 크면 계산 비용이 많이 듭니다.
- 손실 함수가 전역 최소값이 아닌 지역 최소값에 머무를 수 있습니다(로컬 미니마 문제).


## 확률적 경사하강법(Stochastic Gradient Descent, SGD)

- 전체 데이터가 아니라, 무작위로 선택한 일부 데이터(미니배치 또는 단일 샘플)만 사용해 가중치를 업데이트합니다.
- 더 빠르고, 전역 최소값에 도달할 가능성이 높아집니다.


### PyTorch 코드 예시

```python
import torch.optim as optim
optimizer = optim.SGD(model.parameters(), lr=0.01)
```


### 작동 원리 코드 예시

```python
optimizer.zero_grad()   # 기울기 초기화
loss.backward()         # 현재 손실에 대한 기울기 계산
optimizer.step()        # 가중치 업데이트
```


## 에폭(Epoch)

- 모델이 전체 데이터셋을 한 번 완전히 학습하는 과정입니다.
- 여러 번 반복(여러 에폭)하면 모델이 점점 더 데이터를 잘 학습할 수 있습니다.
- 에폭 수가 너무 많으면 과적합(overfitting)이 발생할 수 있습니다.


### 코드 예시

```python
num_epochs = 1000
for epoch in range(num_epochs):
    y = model(x_tensor)
    loss = loss_function(y, t_tensor)
    # ... (이후 optimizer 코드)
```


## 학습률이 클 때의 해결 방안

- 학습률을 낮춰서 안정적으로 학습이 진행되도록 조정합니다.
- 데이터 시각화로 이상치(outlier)를 확인하고 처리합니다.
- 에폭 수를 늘려 충분히 반복 학습합니다.
- 데이터 전처리(예: 표준화)로 입력값의 스케일을 맞춥니다.


## 데이터 표준화

- 입력 데이터와 목표 변수의 값 차이가 클 때, 평균을 0, 분산을 1로 맞추어 표준화하면 모델 학습이 더 잘 됩니다.
- 표준화는 손실 값을 줄이고, 학습의 안정성과 속도를 높일 수 있습니다.


### 코드 예시

```python
from sklearn.preprocessing import StandardScaler

scaler_x = StandardScaler()
x_scaled = scaler_x.fit_transform(x.reshape(-1, 1))
```
